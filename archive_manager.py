"""
ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Supabase Ð² Google Ð¢Ð°Ð±Ð»Ð¸Ñ†Ñ‹
Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‚ Ð¼ÐµÑÑ‚Ð¾ Ð½Ð° Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ð¾Ð¼ Ñ‚Ð°Ñ€Ð¸Ñ„Ðµ Supabase
"""
import os
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

try:
    from supabase_api import get_supabase_api, SupabaseAPI
    from sheets_api import get_sheets_api, SheetsAPI
    IMPORTS_AVAILABLE = True
except ImportError as e:
    IMPORTS_AVAILABLE = False
    logger.error(f"Failed to import required modules: {e}")


@dataclass
class ArchiveConfig:
    """ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸"""
    # Ð’Ð¾Ð·Ñ€Ð°ÑÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸ (Ð´Ð½Ð¸)
    archive_age_days: int = 90
    
    # Ð¢Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸
    tables_to_archive: List[str] = None
    
    # Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°
    batch_size: int = 1000
    
    # Ð£Ð´Ð°Ð»ÑÑ‚ÑŒ Ð»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ÑÐ»Ðµ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸
    delete_after_archive: bool = True
    
    # ID Google Ð¢Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Ð°Ñ€Ñ…Ð¸Ð²Ð°
    archive_sheet_id: Optional[str] = None
    
    def __post_init__(self):
        if self.tables_to_archive is None:
            self.tables_to_archive = [
                'work_log',
                'break_log',
                'work_sessions',
                'violations',
                'sync_log'
            ]
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ID Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸Ð· Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
        if not self.archive_sheet_id:
            self.archive_sheet_id = os.getenv("GOOGLE_ARCHIVE_SHEET_ID") or os.getenv("GOOGLE_SHEET_ID")


@dataclass
class ArchiveStats:
    """Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸"""
    table_name: str
    total_records: int = 0
    archived_records: int = 0
    deleted_records: int = 0
    failed_records: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    @property
    def duration_seconds(self) -> float:
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0


class ArchiveManager:
    """ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    
    def __init__(self, config: Optional[ArchiveConfig] = None):
        if not IMPORTS_AVAILABLE:
            raise ImportError("Required modules not available")
        
        self.config = config or ArchiveConfig()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Google Sheet ID
        if not self.config.archive_sheet_id:
            raise ValueError(
                "GOOGLE_ARCHIVE_SHEET_ID or GOOGLE_SHEET_ID must be set in environment variables"
            )
        
        self.supabase: SupabaseAPI = get_supabase_api()
        self.sheets: SheetsAPI = get_sheets_api()
        self.stats: Dict[str, ArchiveStats] = {}
        
        logger.info(f"âœ… ArchiveManager initialized (age: {self.config.archive_age_days} days)")
    
    def _get_cutoff_date(self) -> datetime:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ñƒ Ð¾Ñ‚ÑÐµÑ‡ÐºÐ¸ Ð´Ð»Ñ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸"""
        return datetime.now(timezone.utc) - timedelta(days=self.config.archive_age_days)
    
    def _get_archive_sheet_name(self, table_name: str) -> str:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð¼Ñ Ð»Ð¸ÑÑ‚Ð° Ð´Ð»Ñ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹"""
        return f"Archive_{table_name}"
    
    def _ensure_archive_sheet(self, sheet_name: str, headers: List[str]) -> None:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð»Ð¸ÑÑ‚ Ð°Ñ€Ñ…Ð¸Ð²Ð° Ð² Google Ð¢Ð°Ð±Ð»Ð¸Ñ†Ðµ ÐµÑÐ»Ð¸ ÐµÐ³Ð¾ Ð½ÐµÑ‚"""
        try:
            # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð»Ð¸ÑÑ‚
            self.sheets.get_worksheet(sheet_name)
            logger.debug(f"Archive sheet '{sheet_name}' already exists")
        except Exception:
            # Ð›Ð¸ÑÑ‚ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚, ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ ÐµÐ³Ð¾
            try:
                # ÐžÑ‚ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ
                if hasattr(self.sheets, '_sheet_cache') and '_spreadsheet' in self.sheets._sheet_cache:
                    spreadsheet = self.sheets._sheet_cache['_spreadsheet']
                else:
                    sheet_id = self.config.archive_sheet_id
                    if not sheet_id:
                        raise ValueError("GOOGLE_ARCHIVE_SHEET_ID or GOOGLE_SHEET_ID not set")
                    spreadsheet = self.sheets.client.open_by_key(sheet_id)
                
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ Ð»Ð¸ÑÑ‚
                ws = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=len(headers))
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸
                self.sheets._request_with_retry(
                    ws.update,
                    'A1',
                    [headers],
                    value_input_option='USER_ENTERED'
                )
                
                logger.info(f"âœ… Created archive sheet '{sheet_name}' with headers: {headers}")
            except Exception as e:
                logger.error(f"Failed to create archive sheet '{sheet_name}': {e}")
                raise
    
    def _convert_record_to_row(self, record: Dict[str, Any], headers: List[str]) -> List[str]:
        """ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¸Ð· Supabase Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð´Ð»Ñ Google Sheets"""
        row = []
        for header in headers:
            value = record.get(header, '')
            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ñ‚Ð¸Ð¿Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            if isinstance(value, datetime):
                value = value.isoformat()
            elif isinstance(value, date):
                value = value.isoformat()
            elif value is None:
                value = ''
            else:
                value = str(value)
            row.append(value)
        return row
    
    def _get_table_headers(self, table_name: str, sample_records: List[Dict]) -> List[str]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸Ð· Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"""
        if not sample_records:
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ðµ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ð´Ð»Ñ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†
            default_headers = {
                'work_log': ['id', 'user_id', 'email', 'name', 'timestamp', 'action_type', 
                            'status', 'details', 'session_id', 'created_at'],
                'break_log': ['id', 'user_id', 'email', 'name', 'break_type', 'start_time',
                             'end_time', 'duration_minutes', 'date', 'status', 'is_over_limit',
                             'session_id', 'created_at', 'updated_at'],
                'work_sessions': ['id', 'session_id', 'user_id', 'email', 'login_time',
                                 'logout_time', 'duration_minutes', 'status', 'logout_type',
                                 'comment', 'created_at', 'updated_at'],
                'violations': ['id', 'user_id', 'email', 'name', 'violation_type', 'break_type',
                              'timestamp', 'expected_duration', 'actual_duration', 'excess_minutes',
                              'date', 'details', 'created_at'],
                'sync_log': ['id', 'sync_type', 'table_name', 'records_processed', 'records_success',
                            'records_failed', 'status', 'error_message', 'started_at', 'completed_at',
                            'duration_seconds']
            }
            return default_headers.get(table_name, [])
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸
        return list(sample_records[0].keys())
    
    def _fetch_old_records(self, table_name: str, cutoff_date: datetime) -> List[Dict[str, Any]]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¸Ð· Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Supabase"""
        try:
            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ð»Ðµ Ð´Ð°Ñ‚Ñ‹ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
            date_fields = {
                'work_log': 'timestamp',
                'break_log': 'date',
                'work_sessions': 'login_time',
                'violations': 'timestamp',
                'sync_log': 'started_at'
            }
            
            date_field = date_fields.get(table_name, 'created_at')
            
            # Ð”Ð»Ñ break_log Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ date (DATE), Ð´Ð»Ñ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… - timestamp
            if table_name == 'break_log':
                # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ datetime Ð² date Ð´Ð»Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ
                cutoff_date_str = cutoff_date.date().isoformat()
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ .lt() Ð´Ð»Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ñ‚
                query = self.supabase.client.table(table_name)\
                    .select('*')\
                    .lt(date_field, cutoff_date_str)
                response = query.execute()
            else:
                # Ð”Ð»Ñ timestamp Ð¿Ð¾Ð»ÐµÐ¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ISO Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                cutoff_date_str = cutoff_date.isoformat()
                query = self.supabase.client.table(table_name)\
                    .select('*')\
                    .lt(date_field, cutoff_date_str)
                # Supabase Ð¼Ð¾Ð¶ÐµÑ‚ Ð¸Ð¼ÐµÑ‚ÑŒ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹, Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¿Ð¾ Ñ‡Ð°ÑÑ‚ÑÐ¼ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
                response = query.execute()
            
            records = response.data if hasattr(response, 'data') else []
            logger.info(f"Found {len(records)} old records in '{table_name}' (older than {cutoff_date.date()})")
            return records
            
        except Exception as e:
            logger.error(f"Failed to fetch old records from '{table_name}': {e}", exc_info=True)
            return []
    
    def _export_to_sheets(self, table_name: str, records: List[Dict[str, Any]]) -> bool:
        """Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² Google Ð¢Ð°Ð±Ð»Ð¸Ñ†Ñƒ"""
        if not records:
            logger.warning(f"No records to export for '{table_name}'")
            return True
        
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸
            headers = self._get_table_headers(table_name, records[:1] if records else [])
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼/Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð»Ð¸ÑÑ‚ Ð°Ñ€Ñ…Ð¸Ð²Ð°
            sheet_name = self._get_archive_sheet_name(table_name)
            self._ensure_archive_sheet(sheet_name, headers)
            
            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² ÑÑ‚Ñ€Ð¾ÐºÐ¸
            rows = []
            for record in records:
                row = self._convert_record_to_row(record, headers)
                rows.append(row)
            
            # Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸
            batch_size = self.config.batch_size
            total_batches = (len(rows) + batch_size - 1) // batch_size
            
            for i in range(0, len(rows), batch_size):
                batch = rows[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                
                try:
                    ws = self.sheets.get_worksheet(sheet_name)
                    self.sheets._request_with_retry(
                        ws.append_rows,
                        batch,
                        value_input_option='USER_ENTERED'
                    )
                    logger.info(f"Exported batch {batch_num}/{total_batches} ({len(batch)} rows) to '{sheet_name}'")
                except Exception as e:
                    logger.error(f"Failed to export batch {batch_num} to '{sheet_name}': {e}")
                    return False
            
            logger.info(f"âœ… Successfully exported {len(records)} records to '{sheet_name}'")
            return True
            
        except Exception as e:
            logger.error(f"Failed to export records to sheets for '{table_name}': {e}")
            return False
    
    def _delete_from_supabase(self, table_name: str, record_ids: List[str]) -> int:
        """Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¸Ð· Supabase Ð¿Ð¾ ID"""
        if not record_ids:
            return 0
        
        try:
            deleted_count = 0
            batch_size = 100  # Supabase Ð¼Ð¾Ð¶ÐµÑ‚ Ð¸Ð¼ÐµÑ‚ÑŒ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ð½Ð° Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
            
            for i in range(0, len(record_ids), batch_size):
                batch_ids = record_ids[i:i + batch_size]
                
                try:
                    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ ID Ð·Ð° Ñ€Ð°Ð· (Supabase Ð¼Ð¾Ð¶ÐµÑ‚ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ñ‚ÑŒ .in_() Ð´Ð»Ñ delete)
                    # Ð˜Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ†Ð¸ÐºÐ» Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ID
                    for record_id in batch_ids:
                        try:
                            self.supabase.client.table(table_name)\
                                .delete()\
                                .eq('id', record_id)\
                                .execute()
                            deleted_count += 1
                        except Exception as e:
                            logger.warning(f"Failed to delete record {record_id} from '{table_name}': {e}")
                    
                    logger.debug(f"Deleted batch {i//batch_size + 1} ({len(batch_ids)} records) from '{table_name}'")
                    
                except Exception as e:
                    logger.error(f"Failed to delete batch from '{table_name}': {e}")
                    # ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼ Ð±Ð°Ñ‚Ñ‡ÐµÐ¼
            
            logger.info(f"âœ… Deleted {deleted_count} records from '{table_name}'")
            return deleted_count
            
        except Exception as e:
            logger.error(f"Failed to delete records from '{table_name}': {e}", exc_info=True)
            return 0
    
    def archive_table(self, table_name: str) -> ArchiveStats:
        """ÐÑ€Ñ…Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ð´Ð½Ñƒ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ"""
        stats = ArchiveStats(table_name=table_name, start_time=datetime.now(timezone.utc))
        self.stats[table_name] = stats
        
        logger.info(f"ðŸ”„ Starting archive for table '{table_name}'")
        
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸
            cutoff_date = self._get_cutoff_date()
            records = self._fetch_old_records(table_name, cutoff_date)
            stats.total_records = len(records)
            
            if not records:
                logger.info(f"âœ… No records to archive for '{table_name}'")
                stats.end_time = datetime.now(timezone.utc)
                return stats
            
            # Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² Google Sheets
            export_success = self._export_to_sheets(table_name, records)
            
            if export_success:
                stats.archived_records = len(records)
                
                # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¸Ð· Supabase ÐµÑÐ»Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¾
                if self.config.delete_after_archive:
                    record_ids = [str(r.get('id', '')) for r in records if r.get('id')]
                    if record_ids:
                        deleted = self._delete_from_supabase(table_name, record_ids)
                        stats.deleted_records = deleted
                else:
                    logger.info(f"âš ï¸  Skipping deletion (delete_after_archive=False)")
            else:
                stats.failed_records = len(records)
                logger.error(f"âŒ Failed to export records for '{table_name}', skipping deletion")
            
            stats.end_time = datetime.now(timezone.utc)
            logger.info(
                f"âœ… Archive completed for '{table_name}': "
                f"{stats.archived_records} archived, "
                f"{stats.deleted_records} deleted, "
                f"{stats.failed_records} failed "
                f"({stats.duration_seconds:.1f}s)"
            )
            
        except Exception as e:
            stats.end_time = datetime.now(timezone.utc)
            stats.failed_records = stats.total_records
            logger.error(f"âŒ Archive failed for '{table_name}': {e}", exc_info=True)
        
        return stats
    
    def archive_all(self) -> Dict[str, ArchiveStats]:
        """ÐÑ€Ñ…Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²ÑÐµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ñ‹Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹"""
        logger.info(f"ðŸš€ Starting archive process for {len(self.config.tables_to_archive)} tables")
        
        all_stats = {}
        for table_name in self.config.tables_to_archive:
            try:
                stats = self.archive_table(table_name)
                all_stats[table_name] = stats
            except Exception as e:
                logger.error(f"Failed to archive table '{table_name}': {e}", exc_info=True)
                all_stats[table_name] = ArchiveStats(
                    table_name=table_name,
                    failed_records=1,
                    start_time=datetime.now(timezone.utc),
                    end_time=datetime.now(timezone.utc)
                )
        
        # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        total_archived = sum(s.archived_records for s in all_stats.values())
        total_deleted = sum(s.deleted_records for s in all_stats.values())
        total_failed = sum(s.failed_records for s in all_stats.values())
        
        logger.info(
            f"ðŸŽ‰ Archive process completed: "
            f"{total_archived} archived, "
            f"{total_deleted} deleted, "
            f"{total_failed} failed"
        )
        
        return all_stats
    
    def get_stats_summary(self) -> Dict[str, Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ²Ð¾Ð´ÐºÑƒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸"""
        if not self.stats:
            return {}
        
        summary = {
            'tables': {},
            'total': {
                'archived': 0,
                'deleted': 0,
                'failed': 0,
                'duration': 0.0
            }
        }
        
        for table_name, stats in self.stats.items():
            summary['tables'][table_name] = {
                'total': stats.total_records,
                'archived': stats.archived_records,
                'deleted': stats.deleted_records,
                'failed': stats.failed_records,
                'duration': stats.duration_seconds
            }
            
            summary['total']['archived'] += stats.archived_records
            summary['total']['deleted'] += stats.deleted_records
            summary['total']['failed'] += stats.failed_records
            summary['total']['duration'] += stats.duration_seconds
        
        return summary


def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸"""
    import sys
    
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('archive.log', encoding='utf-8')
        ]
    )
    
    try:
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ð¸
        config = ArchiveConfig(
            archive_age_days=int(os.getenv("ARCHIVE_AGE_DAYS", "90")),
            delete_after_archive=os.getenv("ARCHIVE_DELETE_AFTER", "1").lower() == "1",
            batch_size=int(os.getenv("ARCHIVE_BATCH_SIZE", "1000"))
        )
        
        manager = ArchiveManager(config)
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð°Ñ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸ÑŽ
        stats = manager.archive_all()
        
        # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        summary = manager.get_stats_summary()
        print("\n" + "="*60)
        print("ARCHIVE SUMMARY")
        print("="*60)
        for table_name, table_stats in summary.get('tables', {}).items():
            print(f"\n{table_name}:")
            print(f"  Total records: {table_stats['total']}")
            print(f"  Archived: {table_stats['archived']}")
            print(f"  Deleted: {table_stats['deleted']}")
            print(f"  Failed: {table_stats['failed']}")
            print(f"  Duration: {table_stats['duration']:.1f}s")
        
        total = summary.get('total', {})
        print(f"\nTOTAL:")
        print(f"  Archived: {total.get('archived', 0)}")
        print(f"  Deleted: {total.get('deleted', 0)}")
        print(f"  Failed: {total.get('failed', 0)}")
        print(f"  Duration: {total.get('duration', 0):.1f}s")
        print("="*60)
        
        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÐºÐ¾Ð´ Ð²Ñ‹Ñ…Ð¾Ð´Ð°
        if total.get('failed', 0) > 0:
            sys.exit(1)
        else:
            sys.exit(0)
            
    except Exception as e:
        logger.error(f"Archive process failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
